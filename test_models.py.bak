# In[ ]:


# coding: utf-8
import os
os.environ["OMP_NUM_THREADS"] = "7" # 7 OpenMP threads + 1 Python thread = 800% CPU util.
#os.environ["CUDA_VISIBLE_DEVICES"] = "1"

import glob
import json
import numpy as np
import PIL.Image
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import utils

import skimage # para o prodes

from Tools import *
import DeepLabV3plus
import modeling.deeplab as d3plus
from scipy.special import softmax



print("NUMBER OF CUDA DEVICES: ", torch.cuda.device_count())

def test(domains, args, global_args, models, file_name):
	device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
	print('Using device:', device)

	patch_size = args.patch_size
	num_classes = args.num_classes
	channels = args.channels

	mini_batch_size = args.batch_size

	weights = args.weights
	gamma = args.gamma

	# large_latent_space = True
	# dilation_rates = ()

	run_test = True
	prodes = args.run_prodes
	# test_only_deforestation = False

	# for threshold
	change_threshold = args.change_threshold
	number_of_points = args.number_of_points


	# output_path = f"./class_output"
	# if (os.path.isdir(output_path) == False):
	# 	os.makedirs(output_path)

	# validation_check = False #Change Path

	# source_domain = global_args.source_domain
	# target_domain = global_args.target_domain


	print("loading data...")

	# domains = {}
	# domains[source_domain] = source
	# domains[target_domain] = target

	file = open(global_args.base_path + '/' + file_name.split('.')[0] + '_' + 
		'test_metrics' + '.txt', 'w')
	    

	print("defining segmentation network...")


	def patch_data(img, isgt = False):
		cut = patch_size
		if (isgt):
			h = img.shape[0]//cut
			w = img.shape[1]//cut
			channel = 1 
			output_img = np.zeros((h*w, cut, cut))
			
		else:
			h = img.shape[1]//cut
			w = img.shape[2]//cut
			channel = channels
			output_img = np.zeros((h*w, channel, cut, cut))
			
		# print(output_img.shape)
		n = 0
		for i in range (h):
			for j in range (w):
				top = cut * i
				bottom = cut * (i+1)
				left = cut * j
				right = cut * (j+1)
				if isgt:
					newImg = img[top:bottom,left:right]
				else:
					newImg = img[:channel, top:bottom,left:right]
				output_img[n] = newImg
				n = n+1
		if(isgt):
			return output_img
		return output_img, h, w


	def rec_gt(patches, h, w):
		# print(patches[0].shape)
		cut = patch_size
		rec = np.zeros((h*cut, w*cut))
		n = 0
		# print(rec.shape)
		for i in range (h):
			for j in range(w):
				top = cut * i
				bottom = cut * (i+1)
				left = cut * j
				right = cut * (j+1)
				rec[top:bottom,left:right] = patches[n]
				n = n+1
		return rec

	def raster_gt(gt):
		new_gt = np.zeros((gt.shape[0],gt.shape[1],3))
		masks = []
		masks.append((gt == 0))
		masks.append((gt == 1))
		# masks.append((gt == 2))
		# masks.append((gt == 3))
		# masks.append((gt == 4))
		# masks.append((gt == 5))

		new_gt[:][masks[0]] = [0,0,0]
		new_gt[:][masks[1]] = [255,255,255]
		# new_gt[:][masks[2]] = [0,255,255]
		# new_gt[:][masks[3]] = [0,255,0]
		# new_gt[:][masks[4]] = [255,255,0]
		# new_gt[:][masks[5]] = [255,0,0] 

		return new_gt

	def calcula_metricass(ref, pred, patch_size):
	  total = 0
	  fp, tp = 0, 0
	  fn, tn = 0, 0
	  c2 = 2
	  c1 = 1
	  c0 = 0

	#  result_image = np.zeros((9, 368,520, 3), dtype = np.float32)

	  for k in range(len(ref)):
	    for i in range(patch_size): #patch_size ou tile_size
	      for j in range(patch_size):  #patch_size ou tile_size
	        if(ref[k,i,j]!=c2):
	          if(ref[k,i,j]==c1):
	            if(pred[k,i,j]==1):
	              tp = tp+1
	      #        result_image[k,i,j] = [0, 255, 255 ] 
	            else:
	              fn = fn+1
	       #       result_image[k,i,j] = [255 ,0 , 0 ] 
	          elif(ref[k,i,j]==c0):
	            if(pred[k,i,j]==0):
	              tn = tn+1
	         #     result_image[k,i,j] = [255, 255, 255 ] 
	            else:
	              fp = fp+1
	         #     result_image[k,i,j] = [0, 0 , 255 ] 
	        else:
	        #  result_image[k,i,j] = [255, 255, 255 ] 
	          total = total+1
	  #  cv2.imwrite("resultados/dissertacao/DLCD-4/4tiles/result"+str(k)+".png", result_image[k])
	          
	  prescision=0.0
	  f1score = 0.0       
	  recall = tp/(tp+fn)
	  if(tp+fp!=0):
	    prescision = tp/(tp+fp)
	  if(prescision!=0):
	    f1score = 100*(2*prescision*recall)/(prescision+recall)
	  P = tp+fn
	  N = tn+fp
	  overall = 100*(tp+tn)/(P+N)
	  alert_rate = 100*(tp+fp)/(P+N)
	  recall = recall * 100
	  prescision = prescision * 100  

	  # print("Total: ", total)
	  # print("tp: ", tp)
	  # print("fp: ", fp)
	  # print("tn: ", tn)
	  # print("fn: ", fn)
	  # print("Overall: %.2f" % overall)
	  # print("F1-Score: %.2f" % f1score)
	  # print("Recall: %.2f" % recall)
	  # print("Prescision: %.2f" % prescision)
	  # print("Alert Rate: %.2f" % alert_rate)
	  # print("Confusion Matrix: \n["+str(tp)+" "+str(fp)+"]\n["+str(fn)+" "+str(tn)+"]")

	  return overall, f1score, recall, prescision, alert_rate 


	# def test():
	# 	torch.utils.data.DataLoader(pa.central_pixels_coor_ts, batch_size = mini_batch_size)


	print ("starting classification...")
	metrics = {}
	for model in (models):
		print(f"model: {model}")
		file.write('--------' + model + ':--------\n')
		# seg_loss_fn = nn.CrossEntropyLoss()
		seg_loss_fn = FocalLoss(weight = torch.FloatTensor(weights).to(device), gamma = gamma)
		classifier = DeepLabV3plus.create(args)
		# classifier = d3plus.DeepLab(num_classes=num_classes,
		#                     backbone='mobilenet',
		#                     output_stride=8
		#                     # sync_bn=args.sync_bn,
		#                     # freeze_bn=args.freeze_bn
		#                     )
		classifier.to(device)
		classifier.load_state_dict(torch.load(models[model]))
		classifier.eval()

		metrics[model] = {}

		with torch.no_grad():
			if run_test:
				print("Running test... ")
				if prodes:
					print("Prodes ON")
				else:
					print("Prodes OFF")

				for domain in (domains):
					print("domain: ", domain)
					file.write(str(type(domain)) + '\n')
					img = utils.channels_last2first(domain.conc_image)
					gt = domain.new_reference
					# domain = 'PA_test'
					metrics[model][domain] = {}
					# metrics[model][domain]["domain_loss"] = 0
					# metrics[model][domain]["domain_acc"] = 0
					# metrics[model][domain]["loss"] = []
					# metrics[model][domain]["acc"] = []
					# metrics[model][domain]["f1"] = []
					# metrics[model][domain]["rec"] = []
					# metrics[model][domain]["prec"] = []
					# metrics[model][domain]["alert"] = []
					# test_loader = torch.utils.data.DataLoader(pa.central_pixels_coor_ts, batch_size = mini_batch_size)
					test_loader = domain.central_pixels_coor_ts
					total_patches = len(domain.central_pixels_coor_ts)
					# print(total_patches)

					test_loader = torch.utils.data.DataLoader(domain.central_pixels_coor_ts, batch_size = mini_batch_size)
					acc, loss, f1, rec, prec, alert  = 0, 0, 0, 0, 0, 0
					count = 0

					if change_threshold:
						min_array = np.zeros((1 , ))
						threshold_list = np.arange(1.0, 0, -1.0/(number_of_points - 1))
						threshold_list = np.concatenate((threshold_list , min_array))
						print(threshold_list)

					# num_classes ignores dont care
					my_test_x = np.zeros((total_patches,num_classes-1,patch_size,patch_size))
					my_test_y = np.zeros((total_patches,patch_size,patch_size))
					this_batch = 0
					for x in (test_loader):
						# x = test_loader
						count += 1
						this_batch_size = x.shape[0]
						coor_batch = x
						x , y = patch_extraction(img, gt, coor_batch, patch_size)
						x = torch.from_numpy(x).float().to(device)
						x = classifier(x)
						y = torch.from_numpy(y).long().to(device)
						loss += seg_loss_fn(x, y).item()
						x = x.cpu().numpy()
						y = y.cpu().numpy()

						x = x[:,:2]
						# x = softmax(x[:,:2], axis = 1)
						print(x.shape)
						# print(x[0,:,0,0])
						# print(sum(x[0,:,0,0]))
						# print(x[0,:,0,1])
						# print(sum(x[0,:,0,1]))
						# print(x[0,:,0,2])
						# print(sum(x[0,:,0,2]))

						
						my_test_x[this_batch:this_batch + this_batch_size] = x
						my_test_y[this_batch:this_batch + this_batch_size] = y
						this_batch += this_batch_size

						# if change_threshold:

						# z1 = x.cpu()
						# y = y.cpu()

						# z2 = (z1[:,1] >= 1.0)
						# print(z2.shape)
						# z3 = z1[:,1] + z1[:,0]
						# z3 = ( (z2[:,0] == True ))
						# z4 = (( z2[:,1] == True))
						# z6 = ((z2[:,2] == True))
						# z5 = z3*1 + z4*1 + z6*1
						# print(np.count_nonzero(z3*1 >= 1.0 ))
						# print(n*3*64*64)
						# print(np.count_nonzero(z4*1))
						# print(np.count_nonzero(z6*1))
						# print(np.count_nonzero(z5 > 1))
						# print(z3.shape)
						# z2 = z2*1
						# z2 = z2.argmax(1)

						# x = x[:,:2].argmax(1)
						# print(x.max())
						# x = x.cpu().numpy()
						# y = y.cpu().numpy()
						# z2 = z2.numpy()

						# z2 = z2.cpu()
						# x[:][(x == 2)] = 0
						# print(type(z2))
						# print(type(x))
						# print("x and z1",np.array_equal(x,z1))
						# print("x and z2",np.array_equal(x,z2))
						# print("z1 and and z2", np.array_equal(z1,z2))
						# x = z2

						# print("unique x", np.unique(x))
						# x[:][(x == 2)] = 0
						# y[:][(y == 2)] = 0


						# if prodes:
						# 	# for i,j in (x,y):
						# 	# print(1)

						# 	# print('x bf', np.count_nonzero(x == 0))
						# 	# print('x bf', np.count_nonzero(x == 1))
						# 	# print('x bf', np.count_nonzero(x == 2))
						# 	x_prodes = skimage.morphology.area_opening(x.astype('int'), area_threshold = 69, connectivity = 1)
						# 	# print('x_prodes', np.count_nonzero(x_prodes == 0))
						# 	# print('x_prodes', np.count_nonzero(x_prodes == 1))
						# 	# print('x_prodes', np.count_nonzero(x_prodes == 2))
						# 	eliminated_samples = x - x_prodes
						# 	# print('eliminated', np.count_nonzero(eliminated_samples == 0))
						# 	# print('eliminated', np.count_nonzero(eliminated_samples == 1))
						# 	# print('eliminated', np.count_nonzero(eliminated_samples == 2))

						# 	# print('y bf', np.count_nonzero(y == 0))
						# 	# print('y bf', np.count_nonzero(y == 1))
						# 	# print('y bf', np.count_nonzero(y == 2))

						# 	y_prodes = y + eliminated_samples
						# 	# print('y_prodes bf', np.count_nonzero(y_prodes == 0))
						# 	# print('y_prodes bf', np.count_nonzero(y_prodes == 1))
						# 	# print('y_prodes bf', np.count_nonzero(y_prodes == 2))
						# 	# print('y_prodes bf', np.count_nonzero(y_prodes == 3))
						# 	# print('y_prodes bf', np.count_nonzero(y_prodes == 4))
						# 	# y_prodes[:][y_prodes == 2] = 0
						# 	# y_prodes[:][y_prodes == 3] = 0

						# 	# y_prodes[:][y_prodes == 3] = 2

						# 	# print('y_prodes af', np.count_nonzero(y_prodes == 0))
						# 	# print('y_prodes af', np.count_nonzero(y_prodes == 1))
						# 	# print('y_prodes af', np.count_nonzero(y_prodes == 2))
						# 	# print('y_prodes af', np.count_nonzero(y_prodes == 3))
						# 	print("y unique",np.unique(y_prodes))
						# 	y = y_prodes

						# else:
						# 	y[:][(y == 2)] = 0

						# new_x = x + 2*y
						# new_x[new_x == 2] = 0
						# new_x[new_x == 3] = 1
						# new_x[new_x == 4] = 0
						# new_x[new_x == 5] = 0
						# new_x[new_x == 6] = 0
						# new_x[new_x == 7] = 0
						# y[:][(y == 2)] = 0
						# y[:][(y == 3)] = 0
						# print("y unique",np.unique(y))
						# print("new_x unique",np.unique(new_x))
						# x = new_x
						
						

						# print(calcula_metricass(y,x,patch_size))
						# print(return_metrics(y,x))
						# a, b, c, d, e = compute_metrics(y,x)
						# print(x.shape)
					
						# a, b, c, d, e = return_metrics(y,x)
						# acc += a
						# f1 += b
						# rec += c
						# prec += d
						# alert += e

					my_test_x = softmax(my_test_x, axis = 1)
					# print(my_test_x.shape)
					# print(my_test_x[0,:,0,0])
					# print(sum(my_test_x[0,:,0,0]))
					# print(my_test_x[0,:,0,1])
					# print(sum(my_test_x[0,:,0,1]))
					# print(my_test_x[0,:,0,2])
					# print(sum(my_test_x[0,:,0,2]))
					# my_test_x = my_test_x.argmax(1)

					if change_threshold:
						metrics_threshold = {}
						metrics_threshold['acc'] = []
						metrics_threshold['f1'] = []
						metrics_threshold['rec'] = []
						metrics_threshold['prec'] = []
						metrics_threshold['alert'] = []

						for threshold in threshold_list:
							temp_x = (my_test_x[:,1] >= threshold)*1
							# print(temp_x.shape)
							# print(np.unique(temp_x))
							acc, f1, rec, prec, alert = return_metrics(my_test_y, temp_x)
							# metrics_threshold['loss'] = loss
							metrics_threshold['acc'].append(acc)
							metrics_threshold['f1'].append(f1)
							metrics_threshold['rec'].append(rec)
							metrics_threshold['prec'].append(prec)
							metrics_threshold['alert'].append(alert)

						with open(global_args.base_path + f"/{model}_{domain}_metrics_threshold.json", "w") as f:
							json.dump(metrics_threshold, f)


					my_test_x = my_test_x.argmax(1)
					# my_test_x = (my_test_x[:,1] >= 1.0)
					acc, f1, rec, prec, alert = return_metrics(my_test_y, my_test_x)
					metrics[model][domain]['loss'] = loss
					metrics[model][domain]['acc'] = acc
					metrics[model][domain]['f1'] = f1
					metrics[model][domain]['rec'] = rec
					metrics[model][domain]['prec'] = prec
					metrics[model][domain]["alert"] = alert
					
					# acc = a
					# f1 = b
					# rec = c
					# prec = d
					# alert = e

						# break
					# print(count)
					# num_slices = 1
					# metrics[model][domain]['loss'] = loss / num_slices
					# metrics[model][domain]['acc'] = acc / num_slices
					# metrics[model][domain]['f1'] = f1 / num_slices
					# metrics[model][domain]['rec'] = rec / num_slices
					# metrics[model][domain]['prec'] = prec / num_slices
					# metrics[model][domain]["alert"] = alert / num_slices

					# file.write('Loss: ' + str(metrics[model][domain]['loss'])+ '\n')
					# file.write('Accuracy: ' + str(metrics[model][domain]['acc'])+ '\n')
					# file.write('F1-score: ' +  str(metrics[model][domain]['f1'])+ '\n')
					# file.write('Recall: ' + str(metrics[model][domain]['rec'])+ '\n')
					# file.write('Precision: ' + str(metrics[model][domain]['prec'])+ '\n')
					# file.write('Alert Rate: ' + str(metrics[model][domain]['alert'])+ '\n')
					# file.write('\n')

					# file.write('Loss: ' + str(metrics[model][domain]['loss'])+ '\n')
					file.write(str(metrics[model][domain]['acc'])+ '\n')
					file.write(str(metrics[model][domain]['f1'])+ '\n')
					file.write(str(metrics[model][domain]['rec'])+ '\n')
					file.write(str(metrics[model][domain]['prec'])+ '\n')
					file.write(str(metrics[model][domain]['alert'])+ '\n')
					file.write('\n')

					print(f"{domain} loss: {metrics[model][domain]['loss']}")
					print(f"{domain} acc: {metrics[model][domain]['acc']}")
					print(f"{domain} f1: {metrics[model][domain]['f1']}")
					print(f"{domain} rec: {metrics[model][domain]['rec']}")
					print(f"{domain} prec: {metrics[model][domain]['prec']}")
					print(f"{domain} alert: {metrics[model][domain]['alert']}")

					# print(f"{domain} loss: {metrics[model][domain]['loss']}")
					# print(f"{domain} acc: {a}")
					# print(f"{domain} f1: {b}")
					# print(f"{domain} rec: {c}")
					# print(f"{domain} prec: {d}")
					# print(f"{domain} alert: {e}")
					# save_folder = f"{output_path}/{model}"
					# if (os.path.isdir(save_folder) == False):
					# 	os.makedirs(save_folder)
					# save_history = f"{save_folder}/{domain}_history.json"
					# with open(save_history, "w") as f:
					# 	json.dump(metrics[model][domain], f)

			else:
				for domain in (data):
					print(f"domain: {domain}")

					img = data[domain]["image"]
					gt = data[domain]["gt"]
					# coor = data[domain]["coor"]

					
					# self.tr_it = 0
					# self.tr_coor = iter(self.train_set)

					# self.img = utils.channels_last2first(data["past"]["image"])
					# self.gt = data["past"]["gt"]

					metrics[model][domain] = {}
					metrics[model][domain]["domain_loss"] = 0
					metrics[model][domain]["domain_acc"] = 0
					metrics[model][domain]["loss"] = []
					metrics[model][domain]["acc"] = []
					metrics[model][domain]["f1"] = []
					metrics[model][domain]["rec"] = []
					metrics[model][domain]["prec"] = []

					# if (validation_check):
					# 	iterations = bootstrap[domain]
					# 	print("Validation data classification")
					# else:
					# iterations = np.arange(len(img))
					print("All data classification")

					# for i in range(len(img)):
					# print(f"img: {i}")

					loss = 0
					acc = 0

					img_patches, h, w = patch_data(img)
					gt_patches = patch_data(gt, isgt = True)
					print(gt_patches.shape)
					classification = np.zeros((gt_patches.shape))
					n = gt_patches.shape[0]

					x = torch.from_numpy(img_patches).float().to(device)
					x = classifier(x)
					y = torch.from_numpy(gt_patches).long().to(device)
					
					loss += seg_loss_fn(x, y).item()
					# x = x.argmax(1)
					# acc += (x == y).sum().item() / (n * patch_size * patch_size)

					x = x.argmax(1)
					x = x.cpu()
					y = y.cpu()
					x = x.reshape(x.shape[0] * x.shape[1] * x.shape[2])
					y = y.reshape(y.shape[0] * y.shape[1] * y.shape[2])
					# print(x.shape)
					# print(y.shape)
					x[:][(x == 2)] = 0
					y[:][(y == 2)] = 0
					# print(np.unique(x))
					# print(np.unique(y))
					acc, f1, rec, prec, conf = compute_metrics(y,x)
					# metrics["seg_acc"] = acc
					# metrics["seg_f1"] = f1
					# metrics["seg_rec"] = rec
					# metrics["seg_prec"] = prec
					# metrics["seg_conf"] = conf

					# x = x.cpu()
					# y = y.cpu()
					# print(type(x[0,0,0]))
					classification = x
					# print(classification)
					print(f"img loss: {loss}")
					print(f"img acc: {acc}")
					metrics[model][domain]["loss"].append(loss)
					metrics[model][domain]["acc"].append(acc)
					metrics[model][domain]["f1"].append(f1)
					metrics[model][domain]["rec"].append(rec)
					metrics[model][domain]["prec"].append(prec)
					# acc, f1, rec, prec, conf = compute_metrics(y.astype(int),x.astype(int))
					
					# metrics[model][domain]["seg_acc"] = acc
					# metrics[model][domain]["seg_f1"] = f1
					# metrics[model][domain]["seg_rec"] = rec
					# metrics[model][domain]["seg_prec"] = prec
					# metrics[model][domain]["seg_conf"] = conf
					
					classification = rec_gt(classification, h, w)
					classification = raster_gt(classification)
					classification = PIL.Image.fromarray(classification.astype('uint8'))
					save_folder = f"{output_path}/{model}"
					if (os.path.isdir(save_folder) == False):
						os.makedirs(save_folder)
					save_path = f"{save_folder}/{domain}_.png"
					classification.save(save_path)
					metrics[model][domain]["domain_loss"] = sum(metrics[model][domain]["loss"])/len(metrics[model][domain]["loss"])
					metrics[model][domain]["domain_acc"] = sum(metrics[model][domain]["acc"])/len(metrics[model][domain]["acc"])
					print(f"{domain} loss: {metrics[model][domain]['domain_loss']}")
					print(f"{domain} acc: {metrics[model][domain]['domain_acc']}")
					print(f"{domain} f1: {metrics[model][domain]['f1']}")
					print(f"{domain} rec: {metrics[model][domain]['rec']}")
					print(f"{domain} prec: {metrics[model][domain]['prec']}")
					save_history = f"{save_folder}/{domain}_history.json"
					with open(save_history, "w") as f:
						json.dump(metrics[model][domain], f)

		del classifier